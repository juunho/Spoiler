{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e22f91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "pd.options.display.float_format = '{:,}'.format\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b3d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_sum_sim(doc_embedding, candidate_embeddings, words, top_n, nr_candidates):\n",
    "    # 문서와 각 키워드들 간의 유사도\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "\n",
    "    # 각 키워드들 간의 유사도\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
    "                                            candidate_embeddings)\n",
    "\n",
    "    # 코사인 유사도에 기반하여 키워드들 중 상위 top_n개의 단어를 pick.\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # 각 키워드들 중에서 가장 덜 유사한 키워드들간의 조합을 계산\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2fdccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # 문서와 각 키워드들 간의 유사도가 적혀있는 리스트\n",
    "    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
    "\n",
    "    # 각 키워드들 간의 유사도\n",
    "    word_similarity = cosine_similarity(candidate_embeddings)\n",
    "\n",
    "    # 문서와 가장 높은 유사도를 가진 키워드의 인덱스를 추출.\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # keywords_idx = [2]\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "\n",
    "    # 가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 ... 중략 ...]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    # 최고의 키워드는 이미 추출했으므로 top_n-1번만큼 아래를 반복.\n",
    "    # ex) top_n = 5라면, 아래의 loop는 4번 반복됨.\n",
    "    for _ in range(top_n - 1):\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # MMR을 계산\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # keywords & candidates를 업데이트\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7ffc7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('key_text.txt', 'r', encoding='UTF-8')\n",
    "doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a7ed54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram 개수 : 567902\n",
      "trigram 다섯개만 출력 : ['00 10 00' '00 agent requirements' '00 agents relative'\n",
      " '00 appointment tony' '00 day life']\n"
     ]
    }
   ],
   "source": [
    "# 3개의 단어 묶음인 단어구 추출\n",
    "n_gram_range = (3, 3)\n",
    "stop_words = \"english\"\n",
    "\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
    "candidates = count.get_feature_names_out()\n",
    "\n",
    "print('trigram 개수 :',len(candidates))\n",
    "print('trigram 다섯개만 출력 :',candidates[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bd57d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6759783f7bfc4b5ab855c9e7846fac22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175a4331c54d4472a57fbe83805c1c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eceb17c0d54b4f5a9cb26f6c493fd4ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.99k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9624552e205a478e9ed6a98b44b03714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/550 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81af4a17b5842039988aa9915da421e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8219038426f44a09b9218363297b8f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c456c9200d40ac9da9f9502b6a5a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6afacf5c101e4cb3b678a4fa4ba3e68b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428beb7768224432bebf81d01ca38e19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4469b70ea954b2ab4c7583560dd2ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f61bd45081247bd81450ccc90d810d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/450 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8d30c241de49d2a5f6f1fa1a92cbaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f33633a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['priest killing rampage', 'alcohol killing sheriff', 'marines kill flamethrower', 'terrorist died yacht', 'loader killing travis']\n"
     ]
    }
   ],
   "source": [
    "top_n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e54aa18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['killed men cancer', 'mobsters knocked unconscious', 'brutally executes sheriff', 'cars explode killing', 'resulting deaths zombies', 'rafters doing suicide', 'explodes killing mccaffrey', 'bridge killing jason', 'killed fleeing battle', 'aliens kill policemen', 'car killing detectives', 'parents murder steve', 'grenade kill zombies', 'torpedoes destroying sickbay', 'explode killing men', 'crash murdering occupants', 'overwhelmed killed zombies', 'alaska killing wolves', 'boat attack smuggler', 'wife killing teddy', 'machete priest killing', 'glider crashed killing', 'chainsaw kills abomination', 'miller kills terrorist', 'suv killing men', 'killing fellow marine', 'gun stole zombie', 'murder ceo major', 'soldiers disintegrated wolverine', 'tumbles shooting terrorists', 'killing dread pirate', 'explosion destroys helicopter', 'guards breaking harley', 'donatellis hostage critically', 'gang killed noodles', 'kicking sharks away', 'rooftops killing police', 'robotics plot killed', 'hospital steals ambulance', 'murder ferry explosion', 'oahu killing men', 'henchman falls dead', 'volunteers kill men', 'destroying ship annihilating', 'killing scientists fight', 'killing sheriff returns', 'mugger aliens kill', 'ship annihilating spiders', 'annihilating spiders professor', 'beanstalk killed husband', 'explosion destabilizes helicopter', 'hunter destroying helicopters', 'crashes killing phelps', 'grenades site killing', 'husband jail destroying', 'hydra parents murder', 'helicopter dead rob', 'motherfucker killing colonel', 'party slaughtered bride', 'grenade killing news', 'men kill girlfriend', 'skulks killing soldier', 'jail destroying cocaine', 'killing detectives knocking', 'malekith killing mother', 'henchman killing instantly', 'hitmen kill driver', 'chest killing decepticon', 'pool kill cops', 'telekinetically wrecking lab', 'explodes killing officers', 'slaughter offending shark', 'bigelow rocketeer murdered', 'shotgun kills guards', 'pastor overrun zombies', 'mayor going hostage', 'killing hydra soldiers', 'destroying helicopters boats', 'grenade kill gangsters', 'hostage killing mother', 'killing fellow sailors', 'scientists kidnapping professor', 'explosion killed batman', 'terrorist wounding gruber', 'robin kills sheriff', 'batman killing father', 'helicopter killing thugs', 'grenade helicopter killing', 'members drown terrorists', 'crashed killing general', 'brutally murder ceo', 'vision killing batman', 'helicopter killing ghost', 'henchwomen kill austin', 'fatally stabbing rocket', 'priest killing rampage', 'alcohol killing sheriff', 'marines kill flamethrower', 'terrorist died yacht', 'loader killing travis']\n"
     ]
    }
   ],
   "source": [
    "top_n = 100\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b5ab020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"keybert_word\", \"wb\") as fp:\n",
    "    pickle.dump(keywords, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd715fe0",
   "metadata": {},
   "source": [
    "### after 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e7be305",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('key_20_text.txt', 'r', encoding='UTF-8')\n",
    "doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c2ead1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram 개수 : 368646\n",
      "trigram 다섯개만 출력 : ['00 10 00' '00 agent requirements' '00 agents relative'\n",
      " '00 appointment tony' '00 day life']\n"
     ]
    }
   ],
   "source": [
    "# 3개의 단어 묶음인 단어구 추출\n",
    "n_gram_range = (3, 3)\n",
    "stop_words = \"english\"\n",
    "\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
    "candidates = count.get_feature_names_out()\n",
    "\n",
    "print('trigram 개수 :',len(candidates))\n",
    "print('trigram 다섯개만 출력 :',candidates[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2e22bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ff6bc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wife priest yells', 'champagne poured creepy', 'partying casino goes', 'huge party suite', 'coveted twinkies fight', 'steward transforms cinderella', 'girl screams tyler', 'brad dance invites', 'downstairs ballroom steals', 'party guests gossip', 'dancers outraged tracy', 'having sex chloe', 'pool flips rage', 'younger brothers fish', 'party night patients', 'sex toilet father', 'hookers trying crash', 'daddy daughter dance', 'party slaughtered bride', 'party stunned announces', 'father wedding underway', 'win basketball scholarship', 'senses pregnant hellboy', 'crew partying rotunda', 'gay friends quickly', 'apartments throwing party', 'party going boys', 'club dancer addicted', 'lacrosse team gay', 'buddies bachelor party', 'starts whacking porn', 'bride party slaughtered', 'party jay angrily', 'billy planning wedding', 'suddenly vampire babies', 'crashed wedding self', 'bathroom forever screaming', 'party gets drunk', 'parties wonder shower', 'frog horror parties', 'jamie graduation party', 'walters flirts wedding', 'party fit panic', 'adam auditioning actresses', 'bachelor party invade', 'ludendorff attending gala', 'bachelor party evening', 'killed cinderella threw', 'dancing partying course', 'swimming race losing', 'coronation party nervous', 'hard partying frank', 'birthday bringing cake', 'adèle movies dance', 'song newly married', 'having big party', 'sisters arrive cake', 'bathtub quick flashback', 'husband kiss husband', 'ravaged cancer gleefully', 'partying frank tank', 'holding big party', 'champagne picks bath', 'bath drinking champagne', 'bullets kicking sharks', 'battle drink mead', 'jeremy crash weddings', 'partygoers forcing survivors', 'stunned wedding ceremony', 'cheering loudest paris', 'graduation party confesses', 'dancing horror velma', 'disappointed everybody kissing', 'gambling dancing partying', 'waltzing maniacally bakehouse', 'promising husband sex', 'baker stumbles cinderella', 'kiss vampire vicious', 'moaning zombie drunk', 'boys sleepwalk attack', 'abusive boyfriend drenched', 'favorite team drunk', 'voting prom king', 'pool retaliation boy', 'dracula dances anna', 'broke lover waiter', 'boyfriend porn producer', 'coming party dad', 'attending parties jocks', 'returning porn industry', 'bachelor party night', 'wedding trying drugs', 'drunk girls miami', 'hobby crashing weddings', 'overnight celebrity senator', 'partying quite loudly', 'family bathroom violent', 'kiss husband shocked', 'chloe yells erik', 'celebrate marriage devastated', 'dancing sex victim', 'toilet flushes wedding', 'final rap battle', 'gay men dance', 'party rushing balcony', 'kissing man wife', 'underwear family dances', 'scream toilet dinner', 'seaweed dancers outraged', 'hysterical woman husband', 'coaches aaron basketball', 'turns gay drunk', 'bridesmaids interrupt hannah', 'wedding water breaks', 'huge drunken brawl', 'cinderella prince infidelity', 'son abducted swimming', 'dinner valentine proposes', 'throw bachelor party', 'wedding crashing rules', 'friend voting prom', 'frog surprise alligator', 'rosies wedding water', 'annie wedding sexual', 'plane bachelorette party', 'getting married weekend', 'waters wedding turner', 'dancing younger brother', 'gay drunk stoned', 'sandman ruthlessly drowning', 'birthday hostesses bathroom', 'inmates brawl breakfast', 'passionate kiss vampire', 'father joins ballet', 'airport kiss drunken', 'valentine impaling swims', 'couple dancing bride', 'party drunk terry', 'pool kill cops', 'partying lots guys', 'party dancing mother', 'throw huge party', 'wedding cinderella prince', 'boyfriend steals mia', 'flirts wedding engagement', 'bachelorette party plans', 'cinderella marries prince', 'party enraged carol', 'bachelorette party hen', 'bachelorette party lillian', 'upcoming dance competition', 'boyfriend drenched blood', 'huge party night', 'bathtub attacking men', 'distracted elaborate wedding', 'married lavish ceremony', 'dance bloodbath needy', 'contract dance incredulous', 'jordan wedding video', 'kingsmen whiskey accidentally', 'party douchebag boyfriend', 'headline drunken billionaire', 'stunned beth bridesmaid', 'swimming lessons competitions', 'alcohol graduation party', 'father kidnapped fairies', 'yelling flirting mobsters', 'wife pool man', 'sex chloe boyfriend', 'olympic game wrestling', 'homecoming dance promising', 'set wedding crashing', 'hannah bachelorette party', 'crashing weddings pickup', 'basketball winning scholarship', 'pool hall drunk', 'wedding couple dancing', 'girl graduation party', 'tony partying casino', 'tears announces boyfriend', 'newly weds dance', 'husband throw party', 'planning shower wedding', 'flushes wedding celebration', 'beats wife pool', 'drinking hard partying', 'huge crush party', 'throws engagement wedding', 'shower wedding annie', 'getting stoned partying', 'juggling drunken college', 'stoned partying lots', 'boy wedding cinderella', 'bachelorettes dating game', 'party dances husband', 'bathtub gushing husband', 'guru wedding crashing', 'security crashed wedding', 'pool party drunk', 'showers singing chloe']\n"
     ]
    }
   ],
   "source": [
    "top_n = 200\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a09e7c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keybert_20_word\", \"wb\") as fp:\n",
    "    pickle.dump(keywords, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b9de8c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 506. GiB for an array with shape (368646, 368646) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12708\\2171999389.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmax_sum_sim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnr_candidates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12708\\1552958982.py\u001b[0m in \u001b[0;36mmax_sum_sim\u001b[1;34m(doc_embedding, candidate_embeddings, words, top_n, nr_candidates)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# 각 키워드들 간의 유사도\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     distances_candidates = cosine_similarity(candidate_embeddings, \n\u001b[1;32m----> 7\u001b[1;33m                                             candidate_embeddings)\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# 코사인 유사도에 기반하여 키워드들 중 상위 top_n개의 단어를 pick.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\juunho\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1257\u001b[0m         \u001b[0mY_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1259\u001b[1;33m     \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_normalized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_normalized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdense_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1261\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\juunho\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     if (\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 506. GiB for an array with shape (368646, 368646) and data type float32"
     ]
    }
   ],
   "source": [
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2d118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8abde970",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 506. GiB for an array with shape (368646, 368646) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12708\\2631346131.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmmr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12708\\757529999.py\u001b[0m in \u001b[0;36mmmr\u001b[1;34m(doc_embedding, candidate_embeddings, words, top_n, diversity)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# 각 키워드들 간의 유사도\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mword_similarity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# 문서와 가장 높은 유사도를 가진 키워드의 인덱스를 추출.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\juunho\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1257\u001b[0m         \u001b[0mY_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1259\u001b[1;33m     \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_normalized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_normalized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdense_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1261\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\juunho\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     if (\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 506. GiB for an array with shape (368646, 368646) and data type float32"
     ]
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85b7f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
